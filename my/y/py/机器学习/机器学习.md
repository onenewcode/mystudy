# 基础
## 机器学习工作流程总结
1.获取数据
2.数据基本处理
3.特征工程
4.机器学习(模型训练)
5.模型评估
## 学习算法
### 监督学习
**定义：**
输入数据是由输入特征值和目标值所组成。
函数的输出可以是一个连续的值(称为回归），
或是输出是有限个离散值（称作分类）。
**例子：**
|  识别信封上手写的邮政编码
|  基于医学影像判断肿瘤是否为良性
|  基于医学影像判断肿瘤是否为良性
### 无监督学习
**定义：**
输入数据是由输入特征值组成，没有目标值
输入数据没有被标记，也没有确定的结果。样本数据类别未知；
需要根据样本间的相似性对样本集进行类别划分。
例子：
|    将客户分成具有相似偏好的群组
|   检测网站的异常访问模式

**定义：**
训练集同时包含有标记样本数据和未标记样本数据。
举例：
### 半监督学习
半监督学习
**定义：**
训练集同时包含有标记样本数据和未标记样本数据。
### 强化学习
**定义：**
实质是make decisions 问题，即自动进行决策，并且可以做连续决策。
举例：
小孩想要走路，但在这之前，他需要先站起来，站起来之后还要保持平衡，接下来还要先迈出一条腿，是左腿还是右腿，迈出一步后还要迈出
下一步。
小孩就是 agent，他试图通过采取行动（即行走）来操纵环境（行走的表面），并且从一个状态转变到另一个状态（即他走的每一步），当他
完成任务的子任务（即走了几步）时，孩子得到奖励（给巧克力吃），并且当他不能走路时，就不会给巧克力。
### 区别
<img src="img/屏幕截图 2022-10-24 111823.png" width = "100%" /> 
<img src="img/屏幕截图 2022-10-24 111857.png" width = "100%"/> 

## 模型评估
### 分类模型评估
- 准确率
   - 预测正确的数占样本总数的比例。
- 其他评价指标：精确率、召回率、F1-score、AUC指标等
### 回归模型评估
- RMSE -- 均方根误差
### 拟合
欠拟合
学习到的东西太少
模型学习的太过粗糙
过拟合
学习到的东西太多
学习到的特征多，不好泛化
## 深度学习简介
### 深度学习 —— 神经网络简介
深度学习（Deep Learning）（也称为深度结构学习【Deep Structured Learning】、层次学习【Hierarchical Learning】或者是深度机器学习【Deep Machine Learning】）是一类算法集合，是机器学习的一个分支。
### 深度学习各层负责内容
1层：负责识别颜色及简单纹理
2层：一些神经元可以识别更加细化的纹理，布纹，刻纹，叶纹等
3层：一些神经元负责感受黑夜里的黄色烛光，高光，萤火，鸡蛋黄色等。
4层：一些神经元识别萌狗的脸，宠物形貌，圆柱体事物，七星瓢虫等的存在。
5层：一些神经元负责识别花，黑眼圈动物，鸟，键盘，原型屋顶等。


## 应用的包和软件
### NumPy
Numpy使用ndarray对象来处理多维数组
#### ndarray介绍
NumPy提供了一个N维数组类型ndarray，它描述了相同类型的“items”的集合。
生成numpy对象:np.array()
ndarray的优势
#### 内存块风格
list -- 分离式存储,存储内容多样化
ndarray -- 一体式存储,存储类型必须一样
ndarray支持并行化运算（向量化运算）
ndarray底层是用C语言写的,效率更高,释放了GIL(全局解释器锁)
#### ndarray特点
<img src="img/屏幕截图 2022-10-24 181753.png" width = "100%" /> 

**生成零一数组**
np.ones(shape, dtype)
np.ones_like(a, dtype)
np.zeros(shape, dtype)
np.zeros_like(a, dtype)
 **生成固定范围的数组**
np.linspace (start, stop, num, endpoint)
**等差数组**
np.linspace (start, stop, num, endpoint)
创建等差数组 — 指定数量
参数:
start:序列的起始值
stop:序列的终止值
num:要生成的等间隔样例数量，默认为50

endpoint:序列中是否包含stop值，默认为ture
step:步长,默认值为1
np.arange(start,stop, step, dtype)
创建等差数组 — 指定步长
参数
step:步长,默认值为1
### Pandas
Pandas中一共有三种数据结构，分别为：Series、DataFrame和MultiIndex
#### DataFrame运算
- 应用add等实现数据间的加、减法运算
    - add(other)
    - sub(other)'
- 应用逻辑运算符号实现数据的逻辑筛选
    - data["open"] > 23
- 应用isin, query实现数据的筛选
    - query(expr)expr:查询字符串
    - isin(values)
- 使用describe完成综合统计
    - 计算平均值、标准差、最大值、最小值
- 使用max, min, mean, std完成统计计算
- 使用idxmin、idxmax完成最大值最小值的索引
- 使用cumsum等实现累计分析
- 应用apply函数实现数据的自定义处理
####  高级处理-缺失值处理
- 应用isnull判断是否有缺失数据NaN
- 应用fillna实现缺失值的填充
    - fillna(value, inplace=True)
        - value:替换成的值
        - inplace:True:会修改原数据，False:不替
- 应用dropna实现缺失值的删除
- 应用replace实现数据的替换
####  高级处理-数据离散化
数据离散化【知道】
可以用来减少给定连续属性值的个数
在连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数值代表落在每个子区间中的属性值。
qcut、cut实现数据分组【知道】
qcut:大致分为相同的几组
cut:自定义分组区间
get_dummies实现哑变量矩阵【知道】
#### 高级处理-合并
应用pd.concat实现数据的合并
应用pd.merge实现数据的合并
#### 高级处理-交叉表与透视表
应用crosstab和pivot_table实现交叉表与透视表
#### 高级处理-分组与聚合
应用groupby和聚合函数实现数据的分组与聚合

### Jupyter Notebook
Jupyter Notebook 是可以在浏览器中运行代码的交互环境。这个工具在探索性数据分析方面
非常有用，在数据科学家中广为使用。虽然 Jupyter Notebook 支持多种编程语言，但我们
只需要支持 Python 即可。用 Jupyter Notebook 整合代码、文本和图像非常方。


#### 使用技巧
<img src="img/屏幕截图 2022-10-24 173613.png" width = "100%" /> 

.2.2 快捷键操作
两种模式通用快捷键
Shift+Enter ，执行本单元代码，并跳转到下一单元
Ctrl+Enter ，执行本单元代码，留在本单元
命令模式：按ESC进入
Y ，cell切换到Code模式
M ，cell切换到Markdown模式
A ，在当前cell的上面添加cell
B ，在当前cell的下面添加cell
其他(了解)
双击D ：删除当前cell
Z ，回退
L ，为当前cell加上行号 <!--
Ctrl+Shift+P ，对话框输入命令直接运行
快速跳转到首个cell， Crtl+Home
快速跳转到最后一个cell， Crtl+End -->
编辑模式：按Enter进入
41
补全代码：变量、方法后跟 Tab键
为一行或多行代码添加/取消注释： Ctrl+/ （Mac:CMD+/）
其他(了解)：
多光标操作： Ctrl键点击鼠标 （Mac:CMD+点击鼠标）
回退： Ctrl+Z （Mac:CMD+Z）
重做： Ctrl+Y （Mac:CMD+Y)
### scikit-learn
scikit-learn 是一个开源项目，可以免费使用和分发，任何人都可以轻松获取其源代码来
查看其背后的原理。scikit-learn 项目正在不断地开发和改进中
### SciPy
SciPy 是 Python 中用于科学计算的函数集合。它具有线性代数高级程序、数学函数优化、
信号处理、特殊数学函数和统计分布等多项功能。scikit-learn 利用 SciPy 中的函数集
合来实现算法。对我们来说，SciPy 中最重要的是 scipy.sparse：它可以给出稀疏矩阵
（sparse matrice），稀疏矩阵是 scikit-learn 中数据的另
### matplotlib
matplotlib 是 Python 主要的科学绘图库，其功能为生成可发布的可视化内容，
**例子**
import matplotlib.pyplot as plt
### 图形绘制流程：
**1.创建画布** -- plt.figure()
plt.figure(figsize=(), dpi=)
figsize:指定图的长宽
dpi:图像的清晰度
返回fig对象
**2.绘制图像** -- plt.plot(x, y)

**3.显示图像** -- plt.show()
```py
import matplotlib.pyplot as plt

# 1.创建画布
plt.figure(figsize=(10, 10), dpi=100)

# 2.绘制折线图
plt.plot([1, 2, 3, 4, 5, 6 ,7], [17,17,18,15,11,11,13])

# 3.显示图像
plt.show()
```
#### 添加自定义x,y刻度
plt.xticks(x, **kwargs)
x:要显示的刻度值
plt.yticks(y, **kwargs)
**y:要显示的刻度值**
增加以下两行代码
 构造x轴刻度标签
x_ticks_label = ["11点{}分".format(i) for i in x]
 构造y轴刻度
y_ticks = range(40)
 修改x,y轴坐标的刻度显示
plt.xticks(x[::5], x_ticks_label[::5])
plt.yticks(y_ticks[::5])
**例子**
```python
# 设置显示中文字体
mpl.rcParams["font.sans-serif"] = ["SimHei"]
# 设置正常显示符号
mpl.rcParams["axes.unicode_minus"] = False
# 0.准备数据
x = range(60)
y_shanghai = [random.uniform(15, 18) for i in x]
# 1.创建画布
plt.figure(figsize=(20, 8), dpi=100)
# 2.绘制图像
plt.plot(x, y_shanghai)
# 2.1 添加x,y轴刻度
# 构造x,y轴刻度标签
x_ticks_label = ["11点{}分".format(i) for i in x]
y_ticks = range(40)
# 刻度显示
plt.xticks(x[::5], x_ticks_label[::5])
plt.yticks(y_ticks[::5])
# 2.2 添加网格显示
plt.grid(True, linestyle="--", alpha=0.5)
# 2.3 添加描述信息
plt.xlabel("时间")
plt.ylabel("温度")
plt.title("中午11点--12点某城市温度变化图", fontsize=20)
# 2.4 图像保存
plt.savefig("./test.png")
# 3.图像显示
plt.show()
```
```python
 0.准备数据
x = range(60)
y_shanghai = [random.uniform(15, 18) for i in x]
y_beijing = [random.uniform(1, 5) for i in x]
# 1.创建画布
# plt.figure(figsize=(20, 8), dpi=100)
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=100)
# 2.绘制图像
# plt.plot(x, y_shanghai, label="上海")
# plt.plot(x, y_beijing, color="r", linestyle="--", label="北京")
axes[0].plot(x, y_shanghai, label="上海")
axes[1].plot(x, y_beijing, color="r", linestyle="--", label="北京")
# 2.1 添加x,y轴刻度
# 构造x,y轴刻度标签
x_ticks_label = ["11点{}分".format(i) for i in x]
y_ticks = range(40)
# 刻度显示
# plt.xticks(x[::5], x_ticks_label[::5])
# plt.yticks(y_ticks[::5])
axes[0].set_xticks(x[::5])
axes[0].set_yticks(y_ticks[::5])
axes[0].set_xticklabels(x_ticks_label[::5])
axes[1].set_xticks(x[::5])
axes[1].set_yticks(y_ticks[::5])
axes[1].set_xticklabels(x_ticks_label[::5])
# 2.2 添加网格显示
# plt.grid(True, linestyle="--", alpha=0.5)
axes[0].grid(True, linestyle="--", alpha=0.5)
axes[1].grid(True, linestyle="--", alpha=0.5)
# 2.3 添加描述信息
# plt.xlabel("时间")
# plt.ylabel("温度")
# plt.title("中午11点--12点某城市温度变化图", fontsize=20)
axes[0].set_xlabel("时间")
axes[0].set_ylabel("温度")
axes[0].set_title("中午11点--12点某城市温度变化图", fontsize=20)
axes[1].set_xlabel("时间")
axes[1].set_ylabel("温度")
axes[1].set_title("中午11点--12点某城市温度变化图", fontsize=20)
# # 2.4 图像保存
plt.savefig("./test.png")
# # 2.5 添加图例
# plt.legend(loc=0)
axes[0].legend(loc=0)
axes[1].legend(loc=0)
# 3.图像显示
plt.show()
```
折线图【知道】
能够显示数据的变化趋势，反映事物的变化情况。(变化)
plt.plot()
散点图【知道】
判断变量之间是否存在数量关联趋势,展示离群点(分布规律)
plt.scatter()
柱状图【知道】
绘制连离散的数据,能够一眼看出各个数据的大小,比较数据之间的差别。(统计/对比)
plt.bar(x, width, align="center")
直方图【知道】
绘制连续性的数据展示一组或者多组数据的分布状况(统计)
plt.hist(x, bins)
饼图【知道】
用于表示不同分类的占比情况，通过弧度大小来对比各种分类
plt.pie(x, labels, autopct, colors)

# K-近邻算法
## 简介
如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。
## 距离度量
1.  欧式距离(Euclidean Distance)
2. 曼哈顿距离(Manhattan Distance)：
在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”(City Block distance)。
3. 切比雪夫距离 (Chebyshev Distance)：
国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？这个距离就叫切比雪夫距离。
4. 闵可夫斯基距离(Minkowski Distance)：
闵氏距离不是一种距离，而是一组距离的定义，是对多个距离度量公式的概括性的表述。
## k值的选择
- K值过小：
    - 容易受到异常点的影响
    - K值的减小就意味着整体模型变得复杂，容易发生过拟合；
- k值过大：
    - 受到样本均衡的问题
    - 与输入实例较远（不相似的）训练实例也会对预测器作用,使预测发生错误，且K值的增大就意味着整体的模型变得简单。
## kd树
k近邻法最简单的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例的距离。计算并存储好以后，再查找K近邻。当训练集很大时，计算非常耗时。
为了提高kNN搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小计算距离的次数。
### kd树简介
kd树：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，如果A和B距离很远，B和C距离很近，那么A和C的距离也很远。有了这个信息，就可以在合适的时候跳过距离远的点。
<img src=".\img\屏幕截图 2023-04-01 141835.png">

1.树的建立；

2.最近邻域搜索（Nearest-Neighbor Lookup）

kd树(K-dimension tree)是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是一种二叉树，表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。

kd树的搜索过程
1.二叉树搜索比较待查询节点和分裂节点的分裂维的值，（小于等于就进入左子树分支，大于就进入右子树分支直到叶子结点）
2.顺着“搜索路径”找到最近邻的近似点
3.回溯搜索路径，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索
4.重复这个过程直到搜索路径为空
```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
# 1、获取鸢尾花数据集
iris = load_iris()
# 对鸢尾花数据集进行分割
# 训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_test
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)
print("x_train:\n", x_train.shape)
# 随机数种子
x_train1, x_test1, y_train1, y_test1 = train_test_split(iris.data, iris.target, random_state=6)
x_train2, x_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, random_state=6)
print("如果随机数种子不一致：\n", x_train == x_train1)
print("如果随机数种子一致：\n", x_train1 == x_train2)
```
## 特征工程-特征预处理
sklearn.preprocessing

### 归一化
通过对原始数据进行变换把数据映射到(默认为[0,1])之间

- sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… )
    - MinMaxScalar.fit_transform(X)
        - X:numpy array格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的array
###   标准化
- sklearn.preprocessing.StandardScaler( )
    - 处理之后每列来说所有数据都聚集在均值0附近标准差差为1
    - StandardScaler.fit_transform(X)
        - X:numpy array格式的数据[n_samples,n_features]
    - 返回值：转换后的形状相同的array
通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内   

对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。

### 案例
- KNeighborsClassifier的使用【知道】
    - sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
        - algorithm（auto,ball_tree, kd_tree, brute） -- 选择什么样的算法进行计算
###  交叉验证，网格搜索
- sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)
- 对估计器的指定参数值进行详尽搜索
- estimator：估计器对象
- param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
- cv：指定几折交叉验证
- fit：输入训练数据
- score：准确率
- 结果分析：
    - bestscore__:在交叉验证中验证的最好结果
    - bestestimator：最好的参数模型
    - cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果
# 线性回归
线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。

- 特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归
## 线性回归api初步使用
- sklearn.linear_model.LinearRegression(fit_intercept=True)
    - 通过正规方程优化
    - 参数
        - fit_intercept：是否计算偏置
    - 属性
        - LinearRegression.coef_：回归系数
        - LinearRegression.intercept_：偏置
- sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)
    - SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。
    - 参数：
    - loss:损失类型
        - loss=”squared_loss”: 普通最小二乘法
    - fit_intercept：是否计算偏置
    - learning_rate : string, optional
        - 学习率填充
        - 'constant': eta = eta0
        - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
        - 'invscaling': eta = eta0 / pow(t, power_t)
            - power_t=0.25:存在父类当中
        - 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。
    - 属性：
        - SGDRegressor.coef_：回归系数
        - SGDRegressor.intercept_：偏置

from sklearn.linear_model import LinearRegression    

## 线性回归的损失和优化
线性回归经常使用的两种优化算法
- 正规方程
- 梯度下降法
​
梯度下降法和正规方程选择依据【知道】
- 小规模数据：
    - 正规方程：LinearRegression(不能解决拟合问题)
    - 岭回归
- 大规模数据：
    - 梯度下降法：SGDRegressor
##  梯度下降法介绍    
- 全梯度下降算法( Full gradient descent）
    - 批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。
- 随机梯度下降算法（Stochastic gradient descent）
    - 每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。
- 小批量梯度下降算法（Mini-batch gradient descent）
    - 每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。
- 随机平均梯度下降算法（Stochastic average gradient descent）
    - 随机平均梯度算法克服了这个问题，在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。
## 欠拟合和过拟合    
- 欠拟合原因以及解决办法
    - 原因：学习到数据的特征过少
    - 解决办法：
        - 添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。
        - 添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。
- 过拟合原因以及解决办法
    - 原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点
    - 解决办法：
        - 1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。
        - 2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。
        - 3）正则化
        - 4）减少特征维度，防止维灾难
## 正则化        
在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化

- L2正则化
    - 作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响
    - 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
    - Ridge回归
- L1正则化
    - 作用：可以使得其中一些W的值直接为0，删除这个特征的影响
    - LASSO回归


- Ridge Regression 岭回归
    - 就是把系数添加平方项
    - 然后限制系数值的大小
    - α值越小，系数值越大，α越大，系数值越小
- Lasso 回归
    - 对系数值进行绝对值处理
    - 由于绝对值在顶点处不可导，所以进行计算的过程中产生很多0，最后得到结果为：稀疏矩阵
E- lastic Net 弹性网络
    - 是前两个内容的综合
    - 设置了一个r,如果r=0--岭回归；r=1--Lasso回归
- Early stopping
    - 通过限制错误率的阈值，进行停止
##     线性回归的改进-岭回归
- sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
    - 具有l2正则化的线性回归
    - alpha:正则化力度，也叫 λ
        - λ取值：0~1 1~10
    - solver:会根据数据自动选择优化方法
        - sag:如果数据集、特征都比较大，选择该随机梯度下降优化
    - normalize:数据是否进行标准化
        - normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据
    - Ridge.coef_:回归权重
    - Ridge.intercept_:回归偏置

Ridge方法相当于SGDRegressor(penalty='l2', loss="squared_loss"),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)

- sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)
    - 具有l2正则化的线性回归，可以进行交叉验证
    - coef_:回归系数
## 模型的保存和加载    
- from sklearn.externals import joblib
    - 保存：joblib.dump(estimator, 'test.pkl')
    - 加载：estimator = joblib.load('test.pkl')

# 逻辑回归
- 逻辑回归的应用场景
    - 广告点击率
    - 是否为垃圾邮件
    - 是否患病
    - 金融诈骗
    - 虚假账号
逻辑回归的输入就是一个线性回归的结果。    
## 损失以及优化
逻辑回归的损失，称之为对数似然损失

**优化**
同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。
## 逻辑回归api介绍
- sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)

    - solver可选参数:{'liblinear', 'sag', 'saga','newton-cg', 'lbfgs'}，

        - 默认: 'liblinear'；用于优化问题的算法。
        - 对于小数据集来说，“liblinear”是个不错的选择，而“sag”和'saga'对于大型数据集会更快。

        - 对于多类问题，只有'newton-cg'， 'sag'， 'saga'和'lbfgs'可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。

    - penalty：正则化的种类

    - C：正则化力度

默认将类别数量少的当做正例

LogisticRegression方法相当于 SGDClassifier(loss="log", penalty=" "),SGDClassifier实现了一个普通的随机梯度下降学习。而使用LogisticRegression(实现了SAG)
## 分类评估方法
###  精确率与召回率

1.  精确率与召回率
- 精确率: 预测结果为正例样本中真实为正例的比例
- 召回率：真实为正例的样本中预测结果为正例的比例（查得全，对正样本的区分能力）
### F1-score
还有其他的评估标准，F1-score，反映了模型的稳健型
### 分类评估报告api
- sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )
    - y_true：真实目标值
    - y_pred：估计器预测目标值
    - labels:指定类别对应的数字
    - target_names：目标类别名称
    - return：每个类别精确率与召回率
### ROC曲线与AUC指标    

- TPR = TP / (TP + FN)
    - 所有真实类别为1的样本中，预测类别为1的比例
- FPR = FP / (FP + TN)
    - 所有真实类别为0的样本中，预测类别为1的比例
#### ROC曲线
 ROC曲线的横轴就是FPRate，纵轴就是TPRate，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5   

 #### AUC指标
- AUC的概率意义是随机取一对正负样本，正样本得分大于负样本得分的概率
- AUC的范围在[0, 1]之间，并且越接近1越好，越接近0.5属于乱猜
AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- 0.5&lt;AUC&gt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。

- from sklearn.metrics import roc_auc_score
    - sklearn.metrics.roc_auc_score(y_true, y_score)
    - 计算ROC曲线面积，即AUC值
    - y_true：每个样本的真实类别，必须为0(反例),1(正例)标记
    - y_score：预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值

# 决策树算法简介
## 决策树分类原理
### 熵
物理学上，熵 Entropy 是“混乱”程度的量度。

系统越有序，熵值越低；系统越混乱或者分散，熵值越高。

**信息理论：**
1、从信息的完整性上进行的描述:

当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。

2、从信息的有序性上进行的描述:

当数据量一致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高。

"信息熵" (information entropy)是度量样本集合纯度最常用的一种指标。
### 决策树的划分依据一----信息增益
信息增益：以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。

信息增益 = entroy(前) - entroy(后)
### 决策树的划分依据二----信息增益率
信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法 不直接使用信息增益，而是使用"增益率" (gain ratio) 来选择最优划分属性.
### 决策树的划分依据三 ----基尼值和基尼指数
CART 决策树 [Breiman et al., 1984] 使用"基尼指数" (Gini index)来选择划分属性.

基尼值Gini（D）：从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。

## cart剪枝
**预剪枝**
（1）每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不再分；

（2）指定树的高度或者深度，例如树的最大深度为4；

（3）指定结点的熵小于某个值，不再划分。随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。
**后剪枝：**
后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。

## 特征工程-特征提取
将任意数据（如文本或图像）转换为可用于机器学习的数字特征



特征提取分类:
字典特征提取(特征离散化)
文本特征提取
图像特征提取（深度学习将介绍）


### 文本特征提取
- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])

    - 返回词频矩阵
    - CountVectorizer.fit_transform(X)
        - X:文本或者包含文本字符串的可迭代对象
        - 返回值:返回sparse矩阵
    - CountVectorizer.get_feature_names() 返回值:单词列表
sklearn.feature_extraction.text.TfidfVectorizer
### jieba分词处理
- jieba.cut()
    - 返回词语组成的生成器
### Tf-idf文本特征提取    

- TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
- TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
## 决策树算法api
- class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)
    - criterion
        - 特征选择标准
        - "gini"或者"entropy"，前者代表基尼系数，后者代表信息增益。一默认"gini"，即CART算法。
    - min_samples_split
        - 内部节点再划分所需最小样本数
        - 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。
    - min_samples_leaf
        - 叶子节点最少样本数
        - 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。
    - max_depth
        - 决策树最大深度
        - 决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间
    - random_state
        -   随机数种子

# 集成学习算法
集成学习通过建立几个模型来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。        
## 集成学习中boosting和Bagging
<img src="./img/屏幕截图 2023-04-01 160614.png">

只要单分类器的表现不太差，集成学习的结果总是要好于单分类器的

## 随机森林和Bagging
### 随机森林构造过程
在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

<img src=".\img\屏幕截图 2023-04-01 161059.png">

随机森林够造过程中的关键步骤（M表示特征数目）


​1. 一次随机选出一个样本，有放回的抽样，重复N次（有可能出现重复的样本）

2. 随机去选出m个特征, m &lt; &lt;M，建立决策树

### 随机森林api介绍

- sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)
    - n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200
    - Criterion：string，可选（default =“gini”）分割特征的测量方法
    - max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30
    - max_features="auto”,每个决策树的最大特征数量
        - If "auto", then max_features=sqrt(n_features).
        - If "sqrt", then max_features=sqrt(n_features)(same as "auto").
        - If "log2", then max_features=log2(n_features).
        - If None, then max_features=n_features.
    - bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样
    - min_samples_split:节点划分最少样本数
    - min_samples_leaf:叶子节点的最小样本数
- 超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf


Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法

经过上面方式组成的集成学习方法:

均可在原有算法上提高约2%左右的泛化正确率

简单, 方便, 通用

##  Boosting
随着学习的积累从弱到强

简而言之：每新加入一个弱学习器，整体能力就会得到提升
<img src="img\屏幕截图 2023-04-01 161600.png">


区别一:数据方面

Bagging：对数据进行采样训练；

Boosting：根据前一轮学习结果调整数据的重要性。

区别二:投票方面

Bagging：所有学习器平权投票；

Boosting：对学习器进行加权投票。

区别三:学习顺序

Bagging的学习是并行的，每个学习器没有依赖关系；

Boosting学习是串行，学习有先后顺序。

区别四:主要作用

Bagging主要用于提高泛化性能（解决过拟合，也可以说降低方差）

Boosting主要用于提高训练精度 （解决欠拟合，也可以说降低偏差）

### GBDT
1.使用梯度下降法优化代价函数；

2.使用一层决策树作为弱学习器，负梯度作为目标值；

3.利用boosting思想进行集成。
# 聚类算法
用户画像，广告推荐，Data Segmentation，搜索引擎的流量推荐，恶意流量识别

基于位置信息的商业推送，新闻聚类，筛选排序

图像分割，降维，识别；离群点检测；信用卡异常消费；发掘相同功能的基因片段

- sklearn.cluster.KMeans(n_clusters=8)
    - 参数:
        - n_clusters:开始的聚类中心数量
            - 整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。
    - 方法:
        - estimator.fit(x)
        - estimator.predict(x)
        - estimator.fit_predict(x)
            - 计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)

## 聚类算法实现流程
- k-means其实包含两层内容：
​   - K : 初始中心点个数（计划聚类数）
​   - means：求中心点到其他数据点距离的平均值
### k-means聚类步骤
1、随机设置K个特征空间内的点作为初始的聚类中心
2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别
3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）
4、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程
## 模型评估
- sse【知道】
    - 误差平方和的值越小越好
- 肘部法【知道】
    - 下降率突然变缓时即认为是最佳的k值
- SC系数【知道】
    - 取值为[-1, 1]，其值越大越好
- CH系数【知道】
    - 分数s高则聚类效果越好
    - CH需要达到的目的：用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。
## 算法优化
k-means算法小结

优点：

​ 1.原理简单（靠近中心点），实现容易

​ 2.聚类效果中上（依赖K的选择）

​ 3.空间复杂度o(N)，时间复杂度o(IKN)

N为样本点个数，K为中心点个数，I为迭代次数
缺点：

​ 1.对离群点，噪声敏感 （中心点易偏移）

​ 2.很难发现大小差别很大的簇及进行增量计算

​ 3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）

### Canopy算法配合初始聚类
优点：

​ 1.Kmeans对噪声抗干扰较弱，通过Canopy对比，将较小的NumPoint的Cluster直接去掉有利于抗干扰。

​ 2.Canopy选择出来的每个Canopy的centerPoint作为K会更精确。

​ 3.只是针对每个Canopy的内做Kmeans聚类，减少相似计算的数量。

缺点：

​ 1.算法中 T1、T2的确定问题 ，依旧可能落入局部最优解
### K-means++

## 特征降维
降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程
### 降维的两种方式
特征选择
主成分分析（可以理解一种特征提取的方式）
### 特征选择
数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。

- Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联
    - 方差选择法：低方差特征过滤   
    - 相关系数
- Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）
    - 决策树:信息熵、信息增益
    - 正则化：L1、L2
    - 深度学习：卷积等

####  低方差特征过滤
- sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
    - 删除所有低方差特征
    - Variance.fit_transform(X)
        - X:numpy array格式的数据[n_samples,n_features]
        - 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。
####    相关系数
皮尔逊相关系数(Pearson Correlation Coefficient)
反映变量之间相关关系密切程度的统计指标
```py
from scipy.stats import pearsonr
x : (N,) array_like
y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)
```
 斯皮尔曼相关系数(Rank IC)
反映变量之间相关关系密切程度的统计指标
from scipy.stats import spearmanr
####  主成分分析

定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。
应用：回归分析或者聚类分析当中

- sklearn.decomposition.PCA(n_components=None)
将数据分解为较低维数空间
n_components:
小数：表示保留百分之多少的信息
整数：减少到多少特征
PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features]
返回值：转换后指定维度的array
