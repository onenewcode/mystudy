{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets \n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 实例化数据库，没有则取torchvision下载\n",
    "data_path = r\"D:\\data\\plane\"\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True) \n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[99] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2431, 0.1961, 0.1804,  ..., 0.6549, 0.7176, 0.5373],\n",
       "         [0.2471, 0.2157, 0.2039,  ..., 0.6392, 0.6706, 0.5686],\n",
       "         [0.2275, 0.2510, 0.2196,  ..., 0.6000, 0.5882, 0.4824],\n",
       "         ...,\n",
       "         [0.6745, 0.5608, 0.5098,  ..., 0.3686, 0.5529, 0.5451],\n",
       "         [0.7176, 0.5882, 0.3137,  ..., 0.3176, 0.5294, 0.5608],\n",
       "         [0.8196, 0.7137, 0.5451,  ..., 0.2314, 0.5098, 0.6627]],\n",
       "\n",
       "        [[0.2510, 0.1961, 0.1725,  ..., 0.6745, 0.7216, 0.5333],\n",
       "         [0.2549, 0.2078, 0.1961,  ..., 0.6627, 0.6824, 0.5725],\n",
       "         [0.2431, 0.2588, 0.2353,  ..., 0.6078, 0.6039, 0.5020],\n",
       "         ...,\n",
       "         [0.5294, 0.4314, 0.2196,  ..., 0.2941, 0.4235, 0.4118],\n",
       "         [0.5725, 0.4627, 0.2510,  ..., 0.2824, 0.4627, 0.4902],\n",
       "         [0.6824, 0.5922, 0.4275,  ..., 0.2118, 0.4667, 0.6118]],\n",
       "\n",
       "        [[0.1725, 0.1020, 0.0745,  ..., 0.2706, 0.2980, 0.2824],\n",
       "         [0.1451, 0.1020, 0.1059,  ..., 0.2392, 0.2941, 0.3020],\n",
       "         [0.1412, 0.1451, 0.1451,  ..., 0.2431, 0.2510, 0.2235],\n",
       "         ...,\n",
       "         [0.3882, 0.3294, 0.1647,  ..., 0.2196, 0.3373, 0.3176],\n",
       "         [0.4588, 0.3725, 0.1725,  ..., 0.2353, 0.3843, 0.4314],\n",
       "         [0.5647, 0.4824, 0.3255,  ..., 0.1843, 0.4353, 0.6275]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms \n",
    "import cv2\n",
    "to_tensor = transforms.ToTensor() \n",
    "img_t = to_tensor(img) \n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, \n",
    " transform=transforms.ToTensor()) \n",
    "img_t, _ = tensor_cifar10[99] \n",
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4914, 0.4822, 0.4465])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# stack（tensors,dim=0,out=None）函数的功能：\n",
    "# 将若干个张量在dim维度上连接,生成一个扩维的张量，比如说原来你有若干个2维张量，连接可以得到一个3维的张量。\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3) \n",
    "# view(3,−1)保留了 3 个通道，并将剩余的所有\n",
    "# 维度合并为一个维度，从而计算出适当的尺寸大小。这\n",
    "# 里我们的 3×32×32 的图像被转换成一个 3×1024 的向量，\n",
    "# 然后对每个通道的 1024 个元素取平均值\n",
    "imgs.view(3, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标准差\n",
    "imgs.view(3, -1).std(dim=1) \n",
    "transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10( \n",
    " data_path, train=True, download=False, \n",
    "transform=transforms.Compose([ \n",
    " transforms.ToTensor(), \n",
    " transforms.Normalize((0.4915, 0.4823, 0.4468), \n",
    " (0.2470, 0.2435, 0.2616)) \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类数据集，进行识别\n",
    "label_map = {0: 0, 2: 1} \n",
    "class_names = ['airplane', 'bird'] \n",
    "cifar2 = [(img, label_map[label]) \n",
    "    for img, label in cifar10 \n",
    "    if label in [0, 2]] \n",
    "cifar2_val = [(img, label_map[label]) \n",
    "    for img, label in cifar10_val \n",
    "    if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "n_out = 2 \n",
    "model = nn.Sequential( \n",
    " nn.Linear( 3072, 512, ), #输入层\n",
    " nn.Tanh(),  #激活函数\n",
    " nn.Linear( 512, n_out, # 输出层，输出类别\n",
    " nn.Softmax(dim=1) )#当dim=1时，指的是在维度1上的元素相加等于1。\n",
    " ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 读取图像\n",
    "img, _ = cifar2[0]\n",
    "# 创建张量转化器\n",
    "transform=transforms.ToTensor()\n",
    "# 将plt格式装换成张量格式\n",
    "img=transform(img)\n",
    "# img=torch.from_numpy(img)\n",
    "# plt.imshow(img) \n",
    "plt.imshow(img.permute(1, 2, 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.205304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(out, torch\u001b[39m.\u001b[39mtensor([label])) \n\u001b[0;32m     28\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \n\u001b[1;32m---> 29\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward() \n\u001b[0;32m     30\u001b[0m     optimizer\u001b[39m.\u001b[39mstep() \n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (epoch, \u001b[39mfloat\u001b[39m(loss)))\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    189\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    190\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    192\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 193\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32md:\\software\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:89\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     88\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format))\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pickletools import optimize\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torchvision import transforms \n",
    "label_map = {0: 0, 2: 1} \n",
    "class_names = ['airplane', 'bird'] \n",
    "cifar2 = [(img, label_map[label]) \n",
    "    for img, label in cifar10 \n",
    "    if label in [0, 2]] \n",
    "cifar2_val = [(img, label_map[label]) \n",
    "    for img, label in cifar10_val \n",
    "    if label in [0, 2]]\n",
    "model = nn.Sequential(\n",
    " nn.Linear(3072, 512),\n",
    " nn.Tanh(), \n",
    " nn.Linear(512, 2), \n",
    " nn.LogSoftmax(dim=1)) \n",
    "learning_rate = 1e-2 \n",
    "optimizer =torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "loss_fn = nn.NLLLoss() \n",
    "n_epochs = 100 \n",
    "transform=transforms.ToTensor()\n",
    "for epoch in range(n_epochs): \n",
    "    for img, label in cifar2: \n",
    "        img=transform(img)\n",
    "        out = model(img.view(-1).unsqueeze(0)) \n",
    "        loss = loss_fn(out, torch.tensor([label])) \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
